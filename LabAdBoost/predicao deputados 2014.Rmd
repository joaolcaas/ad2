---
title: "Predicao Deputados"
author: "João Lucas"
date: "26 de fevereiro de 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(ggplot2)
library(leaps)
library(caret)
library(plotly)
#library(C50)
library(rpart)
library(rpart.plot)
library(ROSE)
```

```{r}
#para lcc
df <- read.csv("/home/joaolaf/Área de Trabalho/train.csv",encoding = "utf-8")

#para notebook
#df <- read.csv("C:\\Users\\João Lucas\\Desktop\\train.csv",encoding = "utf-8")
colnames(df)[13] <- "recursos_pessoas_fisicas"
```


<h3>Nesse lab da disciplina de ad2, queremos prever quais serão os candidatos que serão eleitos na próxima eleição a partir dos dados da eleição de 2014. Para começarmos a predição, vamos primeiro dividir o nosso frame em treino e teste, deixando uma porcentagem de 75% para o treino.</h3>

```{r}
dataPartition <- createDataPartition(y = df$situacao_final,p = 0.75,list = FALSE)

treino <- df[dataPartition,]
teste <- df[-dataPartition,]

```

<h3>1.Há desbalanceamento das classes (isto é, uma classe tem muito mais instâncias que outra)? Em que proporção? Quais efeitos colaterais o desbalanceamento de classes pode causar no classificador? </h3>

  Como queremos saber se os candidatos serão eleitos ou não, vamos fazer a predição com base na situação final de cada candidato na eleição de 2014. No gráfico abaixo, podemos ver que existe uma grande diferença entre a situação eleita e a não eleita.
  
```{r}
a <-treino %>% group_by(situacao_final) %>% 
  summarise(totalAparece = n())

a$indexS <- factor(a$situacao_final, levels = a$situacao_final)

p <- plot_ly(a,x =~indexS,y = ~totalAparece,type = 'bar', name = 'Situações') %>% layout(title = 'Frequência de cada situação',xaxis = list(title=""),yaxis = list(title = "Quantas vezes cada situação aparece"),barmode = 'stack')

p
```
 
  Como isso pode ser prejudicial ao nosso estudo, fazendo com o que o resultado se torne enviezado, vamos tentar ajustar os dados da melhor maneira possível.
  Existem 3 métodos simples para se fazer isso: "up", onde vamos aumentar o parâmetro que está em menor vigor; "down", onde vamos diminuir o parâmetro que está em maior vigor; e o "both", que vai fazer um pouco dos dois.
  Nesse estudo, optei por usar o "both", tendo em vista que se usássemos o "down", restariam pouquíssimos dados para o estudo. Já o "up", vários novos dados teriam que ser criados, o que poderia afetar o no resultado final.


```{r}
treino_balanced <- ovun.sample(situacao_final~., data=treino,p=0.5, seed=1,method="both")$data

table(treino_balanced$situacao_final)

```

```{r}
b <-treino_balanced %>% group_by(situacao_final) %>% 
  summarise(totalApareceBalanced = n())

b$indexS <- factor(b$situacao_final, levels = b$situacao_final)

p_balanced <- plot_ly(b,x =~indexS,y = ~totalApareceBalanced,type = 'bar', name = 'Situações') %>% layout(title = 'Frequência de cada situação',xaxis = list(title=""),yaxis = list(title = "Quantas vezes cada situação aparece"),barmode = 'stack')

p_balanced

```

  Podemos ver que, ao final do processo, os dados não apresentam tanta diferença.
  <h2>Para o resto do processo, vamos usar os dados balanceados.</h2>
  
<h3>2.Treine: um modelo de regressão logística, uma árvore de decisão e um modelo de adaboost. Tune esses modelos usando validação cruzada e controle overfitting se necessário, considerando as particularidades de cada modelo.</h3>

  <h2>Regressão logística</h2>
  

  Para a nossa fórmula, iremos usar os dados que tiveram melhor eficácia quando tentamos predizer o número de votos (lab 3 da disciplina de ad2 -> https://rpubs.com/joaolcaas/predicao_2014_ad2)

```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 10, 
                     verboseIter = FALSE)

formula.votos = as.formula(situacao_final ~  total_receita + total_despesa + quantidade_despesas + recursos_de_pessoas_juridicas + recursos_de_partidos + quantidade_doacoes + quantidade_fornecedores + media_receita + quantidade_doadores)
```


  Para testar a nossa fórmula, vamos usar a regressão logística, que é um técnica que usa conceito similiar ao de regressão linear,porém, com uma diferença de que a variável dependente é uma variável discreta.
  

```{r}
regressaoLogistica <- train(formula.votos,
                 data = treino_balanced,
                 method="glm",
                 trControl = ctrl, 
                 family="binomial",      # se a variável for binária
                 na.action = na.omit)

regressaoLogistica

varImp(regressaoLogistica)

```


  Pela nossa regressão logistica, vimos que os dados mais altos na votacao(como total_receita, que tinha 100 de importancia) desceram bastante e não são mais o mesmo.Além disso podemos ver o valor da Acurácia, que é de 0.8841712. Essa é uma métrica importante quando estamos falando sobre regressão logística pois nos diz a proporção de observações corretamente classificadas. 

  Vamos usar outra formula, adicionando novos atributos, deixando apenas as variáveis mais importantes segundo a regressão logistica

```{r}

formula.situacao.final = as.formula(situacao_final ~ media_receita + quantidade_doacoes + recursos_de_partidos + descricao_cor_raca + despesa_max_campanha + sexo + grau)


regressaoLogisticaNovaFormula <- train(formula.situacao.final,
                 data = treino_balanced,
                 method="glm",
                 trControl = ctrl, 
                 family="binomial",      # se a variável for binária
                 na.action = na.omit)

regressaoLogisticaNovaFormula

```
  Para a nova fórmula, vemos que a Acurácia não se manifestou de forma muito diferente. Sendo assim, vamos continuar usando a fórmula que usa os parâmetros do lab anterior.

  <h2>Árvore de decisão</h2>
  
  Particionando a "árvore", esse modelo gera resultados a partir de parâmetros que demostram se o valor de tal parâmetro leva a um resultado positivo ou negativo. 

```{r}

arvore1 <- train(formula.votos,
                data= treino_balanced, method = "rpart",
                trControl = ctrl,
                cp=0.001,  # parâmetro de complexidade
                maxdepth=20)
arvore1
```

Como o algoritmo seleciona a melhor Acurácia, o valor do do melhor modelo usado é o de cp = 0.004 aproximadamente. Esse modelo nos retornou uma acurácia melhor do que a regressão logística, indicando que os valores podem ser melhor classificados.
  
  <h2>Adaboost</h2>
  
  Assim como as outras técnicas, adaboost vai nos ajudar a predizer quem será eleito, porém, com mais poder, tendo em vista que, a técnica estuda a predição e aumenta os pesos das variáveis que são de mais importância.
  
  Como essa é uma técnica que demanda tempo para ser executada, vamos diminuir o tamanho da formula para o treino.
  
```{r}
formula.adaboost = as.formula(situacao_final ~ media_receita + quantidade_doacoes + recursos_de_partidos)


adaboost <- train(formula.adaboost,
                data=treino_balanced,
                trControl = ctrl,
                method = "adaboost")

adaboost
```

<h3>3.Reporte acurácia, precision, recall e f-measure no treino e validação. Como você avalia os resultados? Justifique sua resposta.</h3>
  
  Como já vimos, acurácia é a proporção de observações corretamente classificadas.
  
  Precision é a porcentagem daqueles de "acerto", ou seja, daqueles que o modelo classificou como certo, quantos realmente eram?
  Recall é a frequência em que o classificador encontra os exemplos de uma classe, ou seja, quanto da classe Y realmente é Y?
  Já o f-measure é a média ponderada da precision e do recall, levando em conta falsos positivos e falsos negativos, sendo o balanço entre esses dois valores.
  

  <h3>No teste</h3>
  
  Primeiro, vamos encontrar os valores para o teste, passando por cada algoritmo.
  
  <h2>Regressão Logística</h2>
```{r}

teste$predicao.reg <- predict(regressaoLogistica, teste)

TP.teste.reg <- teste %>% filter(situacao_final == "eleito", predicao.reg== "eleito") %>% nrow()
TN.teste.reg <- teste %>% filter(situacao_final == "nao_eleito" , predicao.reg == "nao_eleito" ) %>% nrow()
FP.teste.reg <- teste %>% filter(situacao_final == "nao_eleito" , predicao.reg == "eleito") %>% nrow()
FN.teste.reg <- teste %>% filter(situacao_final == "eleito", predicao.reg == "nao_eleito" ) %>% nrow()

accuracy.teste.reg <- (TP.teste.reg + TN.teste.reg)/(TP.teste.reg + TN.teste.reg + FP.teste.reg + FN.teste.reg)
precision.teste.reg <- TP.teste.reg / (TP.teste.reg + FP.teste.reg)
recall.teste.reg <- TP.teste.reg / (TP.teste.reg + FN.teste.reg)
Fmeasure.teste.reg <- 2*(recall.teste.reg*precision.teste.reg)/(recall.teste.reg+precision.teste.reg)

accuracy.teste.reg
precision.teste.reg
recall.teste.reg
Fmeasure.teste.reg

```
  
  
  <h2>Arvore</h2>
  
  
```{r}
teste$predicao.arvore <- predict(arvore1, teste)

TP.teste.arvore <- teste %>% filter(situacao_final == "eleito", predicao.arvore== "eleito") %>% nrow()
TN.teste.arvore <- teste %>% filter(situacao_final == "nao_eleito" , predicao.arvore == "nao_eleito" ) %>% nrow()
FP.teste.arvore <- teste %>% filter(situacao_final == "nao_eleito" , predicao.arvore == "eleito") %>% nrow()
FN.teste.arvore <- teste %>% filter(situacao_final == "eleito", predicao.arvore == "nao_eleito" ) %>% nrow()

accuracy.teste.arvore <- (TP.teste.arvore + TN.teste.arvore)/(TP.teste.arvore + TN.teste.arvore + FP.teste.arvore+ FN.teste.arvore)
precision.teste.arvore <- TP.teste.arvore / (TP.teste.arvore + FP.teste.arvore)
recall.teste.arvore <- TP.teste.arvore / (TP.teste.arvore + FN.teste.arvore)
Fmeasure.teste.arvore <- 2*(recall.teste.arvore*precision.teste.arvore)/(recall.teste.arvore+precision.teste.arvore)

accuracy.teste.arvore
precision.teste.arvore
recall.teste.arvore
Fmeasure.teste.arvore

```
<h2>Adaboost</h2>

```{r}
teste$predicao.adaboost<- predict(adaboost, teste)

TP.teste.adaboost <- teste %>% filter(situacao_final == "eleito", predicao.adaboost == "eleito") %>% nrow()
TN.teste.adaboost <- teste %>% filter(situacao_final == "nao_eleito" , predicao.adaboost == "nao_eleito" ) %>% nrow()
FP.teste.adaboost <- teste %>% filter(situacao_final == "nao_eleito" , predicao.adaboost == "eleito") %>% nrow()
FN.teste.adaboost <- teste %>% filter(situacao_final == "eleito", predicao.adaboost == "nao_eleito" ) %>% nrow()

accuracy.teste.adaboost <- (TP.teste.adaboost + TN.teste.adaboost)/(TP.teste.adaboost + TN.teste.adaboost + FP.teste.adaboost + FN.teste.adaboost)
precision.teste.adaboost <- TP.teste.adaboost / (TP.teste.adaboost + FP.teste.adaboost)
recall.teste.adaboost <- TP.teste.adaboost / (TP.teste.adaboost + FN.teste.adaboost)
Fmeasure.teste.adaboost <- 2*(recall.teste.adaboost*precision.teste.adaboost)/(recall.teste.adaboost+precision.teste.adaboost)

accuracy.teste.adaboost
precision.teste.adaboost
recall.teste.adaboost
Fmeasure.teste.adaboost

```


  <h3>No treino</h3>
  <h2>Regressão logística</h2>
  
```{r}

treino$predicao.reg<- predict(regressaoLogistica, treino)

TP.treino.reg <- treino %>% filter(situacao_final == "eleito", predicao.reg == "eleito") %>% nrow()
TN.treino.reg <- treino %>% filter(situacao_final == "nao_eleito" , predicao.reg == "nao_eleito" ) %>% nrow()
FP.treino.reg <- treino %>% filter(situacao_final == "nao_eleito" , predicao.reg == "eleito") %>% nrow()
FN.treino.reg <- treino %>% filter(situacao_final == "eleito", predicao.reg == "nao_eleito" ) %>% nrow()

accuracy.treino.reg <- (TP.treino.reg + TN.treino.reg)/(TP.treino.reg + TN.treino.reg + FP.treino.reg + FN.treino.reg)
precision.treino.reg <- TP.treino.reg / (TP.treino.reg + FP.treino.reg)
recall.treino.reg <- TP.treino.reg / (TP.treino.reg + FN.treino.reg)
Fmeasure.treino.reg <- 2*(recall.treino.reg*precision.treino.reg)/(recall.treino.reg+precision.treino.reg)

accuracy.treino.reg
precision.treino.reg
recall.treino.reg
Fmeasure.treino.reg
```
  
  <h2>Árvore</h2>
  
    
```{r}
treino$predicao.arvore<- predict(arvore1, treino)

TP.predicao.arvore <- treino %>% filter(situacao_final == "eleito", predicao.arvore == "eleito") %>% nrow()
TN.predicao.arvore <- treino %>% filter(situacao_final == "nao_eleito" , predicao.arvore == "nao_eleito" ) %>% nrow()
FP.predicao.arvore <- treino %>% filter(situacao_final == "nao_eleito" , predicao.arvore == "eleito") %>% nrow()
FN.predicao.arvore <- treino %>% filter(situacao_final == "eleito", predicao.arvore == "nao_eleito" ) %>% nrow()

accuracy.predicao.arvore <- (TP.predicao.arvore + TN.predicao.arvore)/(TP.predicao.arvore + TN.predicao.arvore + FP.predicao.arvore + FN.predicao.arvore)
precision.predicao.arvore <- TP.predicao.arvore / (TP.predicao.arvore + FP.predicao.arvore)
recall.predicao.arvore <- TP.predicao.arvore / (TP.predicao.arvore + FN.predicao.arvore)
Fmeasure.predicao.arvore <- 2*(recall.predicao.arvore*precision.predicao.arvore)/(recall.predicao.arvore+precision.predicao.arvore)

accuracy.predicao.arvore
precision.predicao.arvore
recall.predicao.arvore
Fmeasure.predicao.arvore

```
  
  <h2> Adaboost</h2>
```{r}
treino$predicao.adaboost<- predict(adaboost, treino)

TP.predicao.adaboost <- treino %>% filter(situacao_final == "eleito", predicao.adaboost == "eleito") %>% nrow()
TN.predicao.adaboost <- treino %>% filter(situacao_final == "nao_eleito" , predicao.adaboost == "nao_eleito" ) %>% nrow()
FP.predicao.adaboost <- treino %>% filter(situacao_final == "nao_eleito" , predicao.adaboost == "eleito") %>% nrow()
FN.predicao.adaboost <- treino %>% filter(situacao_final == "eleito", predicao.adaboost == "nao_eleito" ) %>% nrow()

accuracy.predicao.adaboost <- (TP.predicao.adaboost + TN.predicao.adaboost)/(TP.predicao.adaboost + TN.predicao.adaboost + FP.predicao.adaboost + FN.predicao.adaboost)
precision.predicao.adaboost <- TP.predicao.adaboost / (TP.predicao.adaboost + FP.predicao.adaboost)
recall.predicao.adaboost <- TP.predicao.adaboost / (TP.predicao.adaboost + FN.predicao.adaboost)
Fmeasure.predicao.adaboost <- 2*(recall.predicao.adaboost*precision.predicao.adaboost)/(recall.predicao.adaboost+precision.predicao.adaboost)

accuracy.predicao.adaboost
precision.predicao.adaboost
recall.predicao.adaboost
Fmeasure.predicao.adaboost
```

  
  <h3>4.Interprete as saídas dos modelos. Quais atributos parecem ser mais importantes de acordo com cada modelo? Crie pelo menos um novo atributo que não está nos dados originais e estude o impacto desse atributo</h3>  
  
```{r}
varImp(regressaoLogistica)
```
  Mostrando as variáveis mais importantes através do varImp, a regressão logística aponta o que já tínhamos visto na questão 2. A variável media_receita é de extrema importância para esse processo de predição.
  
  <h2>Arvore</h2>
```{r}
varImp(arvore1)
```
  Já na árvore, como foi no nosso lab anterior(), a variável total_receita apresenta 100 de importância. Já media_receita, que foi a melhor variável no modelo acima, mostra que é uma variável importante aqui também, apresentando resultado de aproximadamente 75.
  Porém, total_receita não mostra nenhuma importância na estratégia de regressão logística.
  
  <h2>Adaboost</h2>
```{r}
varImp(adaboost)
```
  
  <h2>Para criar uma nova variável</h2>
  
  Para essa nova variável, vamos testá-la no modelo que mostrou melhor resultado em todo o nosso estudo, regressão logística.
  Como sabemos, regressão logística teve como variável mais importante media_receita. Como queremos novas variáveis, vamos elevar essa ao quadrado, assim como a variável que apresentou segundo melhor resultado, que foi quantidade_doadores. 
```{r}

formula.nova.variavel = as.formula(situacao_final ~ poly(media_receita,2)  + poly(quantidade_doadores,2) + total_receita + total_despesa + quantidade_despesas + recursos_de_pessoas_juridicas + recursos_de_partidos + quantidade_doacoes + quantidade_fornecedores)


regressaoLogisticaNovaVariavel <- train(formula.nova.variavel,
                 data = treino_balanced,
                 method="glm",
                 trControl = ctrl, 
                 family="binomial",      # se a variável for binária
                 na.action = na.omit)

regressaoLogisticaNovaVariavel





```
    Nao mostrou tanta diferença.


  
<h3>Enviando o melhor modelo para o kaggle</h3>


  TEM QUE ENVIAR ID + SITUACAO_FINAL
```{r}
#para lcc
testeKaggle <- read.csv("/home/joaolaf/Área de Trabalho/test.csv",encoding = "utf-8")

#para notebook
#df <- read.csv("C:\\Users\\João Lucas\\Desktop\\train.csv",encoding = "utf-8")
colnames(testeKaggle)[13] <- "recursos_pessoas_fisicas"


regressaoLogistica.kaggle <- train(formula.votos,    #ver qual a melhor formula
                 data = treino_balanced,
                 method="glm",
                 trControl = ctrl, 
                 family="binomial",      # se a variável for binária
                 na.action = na.omit)

reg.final.kaggle <- predict(regressaoLogistica.kaggle,testeKaggle %>% select(-ID))
kaggle.final <- data.frame(ID = testeKaggle$ID, prediction = reg.final.kaggle)
write.csv(kaggle.final,"teste.csv",row.names = F)



```





<h3>Referências</h3>
1.https://shiring.github.io/machine_learning/2017/04/02/unbalanced
2.https://hackinganalytics.files.wordpress.com/2016/09/rare.pdf
  